{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself! - Neural Network\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "The idea is that you improve upon the example Neural Network to increase the ratio of signal ($H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$) to background ($Z, t\\bar{t}, ZZ \\rightarrow \\ell\\ell\\ell\\ell$)\n",
    "\n",
    "First, try to reduce the amount of $Z$ and $t\\bar{t}$ background, since these are quite different to the signal.\n",
    "\n",
    "Then, try to reduce the amount of $ZZ \\rightarrow \\ell\\ell\\ell\\ell$, whilst keeping $H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$ signal\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker.\n",
    "\n",
    "This notebook was inspired by https://www.springboard.com/blog/beginners-guide-neural-network-in-python-scikit-learn-0-18/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../HZZ_feynman.pdf\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you re-open on binder, you will need to run this cell again.\n",
    "\n",
    "If you run into a problem of \"uproot not being available\", Kernel -> Restart & Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "!{sys.executable} -m pip install -U numpy pandas uproot matplotlib keras scikit-learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import infofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 1000\n",
    "                                                                                                                                  \n",
    "tuple_path = \"Input/\"\n",
    "\n",
    "stack_order = ['data',r'$Z,t\\bar{t}$','ZZ',r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['DataEgamma','DataMuons']\n",
    "    },\n",
    "\n",
    "    r'$Z,t\\bar{t}$' : {\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#8700da\"\n",
    "    },\n",
    "\n",
    "    'ZZ' : {\n",
    "        'list' : ['ZZ'],\n",
    "        'color' : \"#f90000\"\n",
    "    },\n",
    "\n",
    "    r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' : {\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep'],\n",
    "        'color' : \"#4faeff\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in samples:\n",
    "        print(s+':')\n",
    "        frames = []\n",
    "        for val in samples[s]['list']:\n",
    "            prefix = \"MC/skim.mc_\"\n",
    "            if s == 'data':\n",
    "                prefix = \"Data/skim.\"\n",
    "            else: prefix += str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".root\"\n",
    "            print(fileString)\n",
    "            f = glob.glob(fileString,recursive=False)[0]\n",
    "            if f != \"\":\n",
    "                temp = read_file(f,val)\n",
    "                frames.append(temp)\n",
    "            else:\n",
    "                print(\"Error: \"+val+\" not found!\")\n",
    "        data[s] = pd.concat(frames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mllll_window(mllll):\n",
    "    return 120 < mllll < 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,\n",
    "                scaleFactor_MUON, scaleFactor_TRIGGER):\n",
    "    return mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_TRIGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(totalWeight,sample):\n",
    "    info = infofile.infos[sample]\n",
    "    weight = (lumi*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"])\n",
    "    weight *= totalWeight\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(data, title, **kwds):\n",
    "    \"\"\"Calculate pairwise correlation between features.\n",
    "    \n",
    "    Extra arguments are passed on to DataFrame.corr()\n",
    "    \"\"\"\n",
    "    # simply call df.corr() to get a table of\n",
    "    # correlation values if you do not need\n",
    "    # the fancy plotting\n",
    "    corrmat = data.corr(**kwds)\n",
    "\n",
    "    fig, ax1 = plt.subplots(ncols=1, figsize=(6,5))\n",
    "    \n",
    "    opts = {'cmap': plt.get_cmap(\"RdBu\"),\n",
    "            'vmin': -1, 'vmax': +1}\n",
    "    heatmap1 = ax1.pcolor(corrmat, **opts)\n",
    "    plt.colorbar(heatmap1, ax=ax1)\n",
    "\n",
    "    ax1.set_title(title+\" Correlations\")\n",
    "\n",
    "    labels = corrmat.columns.values\n",
    "    for ax in (ax1,):\n",
    "        # shift location of ticks to center of the bins\n",
    "        ax.set_xticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_yticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_xticklabels(labels, minor=False, ha='right', rotation=70)\n",
    "        ax.set_yticklabels(labels, minor=False)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "\n",
    "    bins = [80 + x*5 for x in range(35) ]\n",
    "    data_x = [82.5 + x*5 for x in range(34) ]\n",
    "\n",
    "    data_mllll = []\n",
    "    data_mllll_errors = []\n",
    "\n",
    "    mc_mllll = []\n",
    "    mc_weights = []\n",
    "    mc_colors = []\n",
    "    mc_labels = []\n",
    "    mc_in_mllll_window = [] # list for numbers of MC events with 120 < mllll < 130 GeV\n",
    "\n",
    "    for s in stack_order:\n",
    "        if s == \"data\":\n",
    "            data_mllll,_ = np.histogram(data[s].mllll.values, bins=bins)\n",
    "            data_mllll_errors = np.sqrt(data_mllll)\n",
    "        else:\n",
    "            mc_labels.append(s)\n",
    "            mc_mllll.append(data[s].mllll.values)\n",
    "            mc_colors.append(samples[s]['color'])\n",
    "            mc_weights.append(data[s].totalWeight.values)\n",
    "            mc_in_mllll_window.append([data[s].totalWeight.values[mllll_iter] for mllll_iter in range(len(data[s].mllll.values)) if 120 < data[s].mllll.values[mllll_iter] < 130])\n",
    "    \n",
    "    HZZ_in_mllll_window = sum(mc_in_mllll_window[2]) # number signal MC events with 120 < mllll < 130 GeV\n",
    "    background_in_mllll_window = sum(mc_in_mllll_window[0]+mc_in_mllll_window[1]) # number background MC events with 120 < mllll < 130 GeV\n",
    "    SoversqrtB = HZZ_in_mllll_window/math.sqrt(background_in_mllll_window) # calculate significance\n",
    "    print('Signal/sqrt(Background) for 120<mllll<130 '+str(SoversqrtB))\n",
    "    \n",
    "    top = np.amax(data_mllll)+math.sqrt(np.amax(data_mllll))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(mc_mllll,bins=bins,weights=mc_weights,stacked=True,color=mc_colors, label=mc_labels)\n",
    "    plt.errorbar( x=data_x, y=data_mllll, yerr=data_mllll_errors, fmt='ko', label='Data')\n",
    "\n",
    "    #X = np.arange(115,135,5) # gives list [115,120,125,130]\n",
    "    #data_in_window = data_mllll[7:11] # gives list of data y value for [115,120,125,130]\n",
    "    #x = np.sum(X*data_in_window)/np.sum(data_in_window) # Gaussian mean\n",
    "    #width = np.sqrt(np.abs(np.sum((X-x)**2*data_in_window)/np.sum(data_in_window))) # Gaussian width\n",
    "    #fit = lambda t : np.amax(data_in_window)*np.exp(-(t-x)**2/(2*width**2)) # Gaussian fit\n",
    "    #plt.plot(X, fit(X), '-') # plot Gaussian\n",
    "    \n",
    "    plt.xlabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "\n",
    "    plt.ylabel(r'Events',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.yscale('log')                                                                                                                                                                        \n",
    "    plt.ylim(bottom=0,top=top)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    plt.text(0.05,0.97,r'$\\mathbf{{ATLAS}}$ Open Data',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,fontsize=13)\n",
    "    plt.text(0.05,0.92,'for education only',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,style='italic',fontsize=8)\n",
    "    plt.text(0.05,0.9,r'$\\sqrt{s}=8\\,\\mathrm{TeV},\\;\\int L\\,dt=1\\,\\mathrm{fb}^{-1}$',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"plot.pdf\")\n",
    "    \n",
    "    # scatter plot of signal and background lep_n vs mllll\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': plt.scatter(data[s].lep_n,data[s].mllll,color=samples[s]['color'],label=s)\n",
    "    #plt.xlabel(r'Leptons',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.ylabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "    #plt.legend()\n",
    "    \n",
    "    # scatter plot of signal and background lep_pt[0] vs mllll\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': plt.scatter(data[s].lep_pt.apply(lambda x: x[0]),data[s].mllll,color=samples[s]['color'],label=s)\n",
    "    #plt.legend()\n",
    "    \n",
    "    # boxplot of signal and background mllll\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': data[s]['Process'] = s\n",
    "    #data_all = pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ'],data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']])\n",
    "    #data_all.boxplot(by='Process',column=[\"mllll\"])\n",
    "    \n",
    "    # plot correlation matrices for signal and background\n",
    "    #correlations(pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ']]),\"background\")\n",
    "    #correlations(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'], \"signal\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pts,lep_etas,lep_phis):\n",
    "    theta_0 = 2*math.atan(math.exp(-lep_etas[0]))\n",
    "    theta_1 = 2*math.atan(math.exp(-lep_etas[1]))\n",
    "    theta_2 = 2*math.atan(math.exp(-lep_etas[2]))\n",
    "    theta_3 = 2*math.atan(math.exp(-lep_etas[3]))\n",
    "    p_0 = lep_pts[0]/math.sin(theta_0)\n",
    "    p_1 = lep_pts[1]/math.sin(theta_1)\n",
    "    p_2 = lep_pts[2]/math.sin(theta_2)\n",
    "    p_3 = lep_pts[3]/math.sin(theta_3)\n",
    "    pz_0 = p_0*math.cos(theta_0)\n",
    "    pz_1 = p_1*math.cos(theta_1)\n",
    "    pz_2 = p_2*math.cos(theta_2)\n",
    "    pz_3 = p_3*math.cos(theta_3)\n",
    "    px_0 = p_0*math.sin(theta_0)*math.cos(lep_phis[0])\n",
    "    px_1 = p_1*math.sin(theta_1)*math.cos(lep_phis[1])\n",
    "    px_2 = p_2*math.sin(theta_2)*math.cos(lep_phis[2])\n",
    "    px_3 = p_3*math.sin(theta_3)*math.cos(lep_phis[3])\n",
    "    py_0 = p_0*math.sin(theta_0)*math.sin(lep_phis[0])\n",
    "    py_1 = p_1*math.sin(theta_1)*math.sin(lep_phis[1])\n",
    "    py_2 = p_2*math.sin(theta_2)*math.sin(lep_phis[2])\n",
    "    py_3 = p_3*math.sin(theta_3)*math.sin(lep_phis[3])\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3\n",
    "    sumE = p_0 + p_1 + p_2 + p_3\n",
    "    mllll = sumE**2 - sumpz**2 - sumpx**2 - sumpy**2\n",
    "    return math.sqrt(mllll)/1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mll(lep_pts,lep_etas,lep_phis):\n",
    "    # this is only pseudo-code to tell you what to do!\n",
    "    # you need to decide how to find i & j yourself\n",
    "    mll = 2*lep_pts[i]*lep_pts[j!=i]\n",
    "    cosh = math.cosh(lep_etas[i]-lep_etas[j!=i])\n",
    "    cos = math.cos(lep_phis[i]-lep_phis[j!=i])\n",
    "    mll *= ( cosh - cos )\n",
    "    return math.sqrt(mll)/1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncommenting a new cut\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time()\n",
    "    print(\"\\tProcessing: \"+sample)\n",
    "    mc = uproot.open(path)[\"mini\"]\n",
    "    data = mc.pandas.df([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_charge\",\"lep_type\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\",\n",
    "                         \"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\", # add more variables here if you make cuts on them\n",
    "                         \"scaleFactor_TRIGGER\"], flatten=False)\n",
    "\n",
    "    nIn = len(data.index)\n",
    "\n",
    "    if 'Data' not in sample:\n",
    "        data['totalWeight'] = np.vectorize(calc_weight)(data.mcWeight,data.scaleFactor_PILEUP,data.scaleFactor_ELE,data.scaleFactor_MUON,data.scaleFactor_TRIGGER)\n",
    "        data['totalWeight'] = np.vectorize(get_xsec_weight)(data.totalWeight,sample)\n",
    "\n",
    "    data.drop([\"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\",\"scaleFactor_TRIGGER\"], axis=1, inplace=True)\n",
    "    \n",
    "    # cut on minimum lepton pt\n",
    "    \n",
    "    # cut on lepton etcone20\n",
    "    \n",
    "    # cut on lepton d0\n",
    "    \n",
    "    # example of adding column that takes the return of the function cut_lep_pt_min\n",
    "    #data['lep_pt_min'] = data.apply(cut_lep_pt_min,axis=1)\n",
    "    \n",
    "    # example of cut on minimum number of leptons passing baseline requirements\n",
    "    #fail = data[ np.vectorize(cut_n_lep_min)(data.lep_pt_min) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on number of leptons\n",
    "    fail = data[ np.vectorize(cut_n_lep)(data.lep_n) ].index\n",
    "    data.drop(fail, inplace=True)\n",
    "\n",
    "    # cut on lepton charge\n",
    "    #fail = data[ np.vectorize(cut_lep_charge)(data.lep_charge) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # cut on lepton type\n",
    "    #fail = data[ np.vectorize(cut_lep_type)(data.lep_type) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on lepton pt\n",
    "    #fail = data[ np.vectorize(cut_lep_pt)(data.lep_pt) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on deltaR\n",
    "    #fail = data[ np.vectorize(cut_deltaR)(data.lep_eta,data.lep_phi...\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "    #fail = data[ np.vectorize(cut_OCST)(data....\n",
    "\n",
    "    # calculation of Z boson candidate 1 invariant mass\n",
    "    #data['mZ1'] = np.vectorize(calc_mZ1)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "    \n",
    "    # cut on mZ1\n",
    "    #fail = data[ np.vectorize(cut_mZ1)(data.mZ1) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of Z boson candidate 2 invariant mass\n",
    "    #data['mZ2'] = np.vectorize(calc_mZ2)(data....\n",
    "    \n",
    "    # cut on mZ2\n",
    "    #fail = data[ np.vectorize(cut_mZ2)(data.mZ2) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of 4-lepton invariant mass\n",
    "    data['mllll'] = np.vectorize(calc_mllll)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "\n",
    "    mllll_window_list = data[ np.vectorize(mllll_window)(data.mllll) ] # return events with 120 < mllll < 130 GeV\n",
    "    \n",
    "    # example of expanding lep_pt list column into individual columns whilst requiring exactly 4 leptons\n",
    "    # need to change cut_lep_n to require exactly 4 leptons\n",
    "    #data[['lep1_pt','lep2_pt','lep3_pt','lep4_pt']] = pd.DataFrame(data.lep_pt.values.tolist(), index= data.index)\n",
    "    #data[['lep1_eta','lep2_eta','lep3_eta','lep4_eta']] = pd.DataFrame(data.lep_eta.values.tolist(), index= data.index)\n",
    "    #data[['lep1_phi','lep2_phi','lep3_phi','lep4_phi']] = pd.DataFrame(data.lep_phi.values.tolist(), index= data.index)\n",
    "\n",
    "    # example of expanding lep_pt list column into individual columns without requiring exactly 4 leptons\n",
    "    # need to do this for columns that you wish to use for fit_BDT\n",
    "    #if max(data.lep_n) < 5: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt'], index=data.index)\n",
    "    #    df_split['lep5_pt'] = 0\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #elif max(data.lep_n) < 6: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt'], index=data.index)\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #else: df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt','lep6_pt'], index=data.index)\n",
    "    #df_split.fillna(0, inplace=True)\n",
    "    #data = pd.concat([data, df_split], axis=1)\n",
    "    \n",
    "    #if max(data.lep_n) < 5: \n",
    "    #    df_split = pd.DataFrame(data['lep_eta'].values.tolist(), columns=['lep1_eta','lep2_eta','lep3_eta','lep4_eta'], index=data.index)\n",
    "    #    df_split['lep5_eta'] = 0\n",
    "    #    df_split['lep6_eta'] = 0\n",
    "    #elif max(data.lep_n) < 6: \n",
    "    #    df_split = pd.DataFrame(data['lep_eta'].values.tolist(), columns=['lep1_eta','lep2_eta','lep3_eta','lep4_eta','lep5_eta'], index=data.index)\n",
    "    #    df_split['lep6_eta'] = 0\n",
    "    #else: df_split = pd.DataFrame(data['lep_eta'].values.tolist(), columns=['lep1_eta','lep2_eta','lep3_eta','lep4_eta','lep5_eta','lep6_eta'], index=data.index)\n",
    "    #df_split.fillna(0, inplace=True)\n",
    "    #data = pd.concat([data, df_split], axis=1)\n",
    "    \n",
    "    #if max(data.lep_n) < 5: \n",
    "    #    df_split = pd.DataFrame(data['lep_phi'].values.tolist(), columns=['lep1_phi','lep2_phi','lep3_phi','lep4_phi'], index=data.index)\n",
    "    #    df_split['lep5_phi'] = 0\n",
    "    #    df_split['lep6_phi'] = 0\n",
    "    #elif max(data.lep_n) < 6: \n",
    "    #    df_split = pd.DataFrame(data['lep_phi'].values.tolist(), columns=['lep1_phi','lep2_phi','lep3_phi','lep4_phi','lep5_phi'], index=data.index)\n",
    "    #    df_split['lep6_phi'] = 0\n",
    "    #else: df_split = pd.DataFrame(data['lep_phi'].values.tolist(), columns=['lep1_phi','lep2_phi','lep3_phi','lep4_phi','lep5_phi','lep6_phi'], index=data.index)\n",
    "    #df_split.fillna(0, inplace=True)\n",
    "    #data = pd.concat([data, df_split], axis=1)\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # if you want a variable to be input into your BDT, remove it from the drop function\n",
    "    data.drop([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_charge\",\"lep_type\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\"], axis=1, inplace=True)  \n",
    "    \n",
    "    # totalWeight needs to be dropped if you want to do bdt\n",
    "    # if this is done, need to comment out plot_data(data) in bottom cell\n",
    "    if 'Data' not in sample: data.drop([\"totalWeight\"], axis=1, inplace=True)\n",
    "\n",
    "    nOut = len(data.index)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\"\\t\\tTime taken: \"+str(elapsed)+\", nIn: \"+str(nIn)+\", nOut: \"+str(nOut))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing an already uncommented cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change a cut: Cell -> Run All Below\n",
    "\n",
    "If you uncomment a cut here, you also need to uncomment the corresponding cut in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut on number of leptons\n",
    "def cut_n_lep(lep_n):\n",
    "    # return when number of leptons is less than 4\n",
    "    return lep_n != 4\n",
    "\n",
    "# cut on lepton charge\n",
    "def cut_lep_charge(lep_charge):\n",
    "    # return when sum of lepton charges is not equal to 0\n",
    "    # exclamation mark (!) means \"not\"\n",
    "    # so != means \"not equal to\"\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    return lep_charge[0] + lep_charge[1] + lep_charge[2] + lep_charge[3] != 0\n",
    "\n",
    "# cut on lepton type\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "    sum_lep_type = lep_type[0] + lep_type[1] + lep_type[2] + lep_type[3]\n",
    "    return (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 44) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 48) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 52)\n",
    "\n",
    "# cut on lepton pt\n",
    "#def cut_lep_pt(lep_pt):\n",
    "# want to throw away events where the 2nd highest pt lepton used has lep_pt[1] < 15000\n",
    "# want to throw away events where the 3rd highest pt lepton used has lep_pt[2] < 10000\n",
    "\n",
    "# cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "#def cut_mOCST():\n",
    "# want to throw away events if the invariant mass of any opposite-charge-same-type lepton pair is < 5\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 1\n",
    "#def cut_mZ1(mZ1):\n",
    "# want invariant mass of same-type-opposite-charge lepton pair that's closest to Z mass (91 GeV) to be in range 50 < m < 106 GeV\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 2\n",
    "#def cut_mZ2(mZ2):\n",
    "# want invariant mass of remaining lepton pair that's closest to Z mass (91 GeV) to be in range 17.5 < m < 115 GeV\n",
    "# advanced: vary the lower range monotically from 17.5 at mllll=120 to 50 at mllll=190, and constant above mllll=190\n",
    "\n",
    "# cut on deltaR\n",
    "# want to throw away leptons that are separated from all other leptons by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.2\n",
    "# want to throw away leptons that are separated from other leptons of the same type by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.1\n",
    "\n",
    "# example of returning list where every element passes minimum lep_pt requirement\n",
    "#def cut_lep_pt_min(data):\n",
    "#    return [data.lep_pt[i] for i in range(len(data.lep_pt)) if data.lep_pt[i] > 6000]\n",
    "\n",
    "# cut on minimum lepton pt\n",
    "# want to throw away muons with lep_pt < 6000\n",
    "# want to throw away electrons with lep_pt < 7000\n",
    "\n",
    "# cut on maximum lepton etcone20\n",
    "# want to throw away muons with lep_etcone20/lep_pt < 0.3\n",
    "# want to throw away electrons with lep_etcone20/lep_pt < 0.2\n",
    "\n",
    "# cut on maximum lepton d0\n",
    "# want to throw away muons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 3.5\n",
    "# want to throw away electrons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 6.5\n",
    "\n",
    "# example of cutting on length of list passing minimum requirements\n",
    "#def cut_n_lep_min(lep_pt_min):\n",
    "#    return len(lep_pt_min) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start = time.time()\n",
    "    data = get_data_from_files()\n",
    "    #plot_data(data)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Time taken: \"+str(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set up our Data and our Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add isSignal variable\n",
    "data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']['isSignal'] = np.ones(len(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'])) \n",
    "data[r'$Z,t\\bar{t}$']['isSignal'] = np.zeros(len(data[r'$Z,t\\bar{t}$']))\n",
    "data['ZZ']['isSignal'] = np.zeros(len(data['ZZ']))\n",
    "data_all = pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ'],data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']])\n",
    "\n",
    "X = data_all.drop('isSignal',axis=1)\n",
    "y = data_all['isSignal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "Let’s split our data into training and testing sets, this is done easily with SciKit Learn’s train_test_split function from model_selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The neural network in Python may have difficulty converging before the maximum number of iterations allowed if the data is not normalized. Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. Note that you must apply the same scaling to the test set for meaningful results. There are a lot of different methods for normalization of data, we will use the built-in StandardScaler for standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Now it is time to train our model. SciKit Learn makes this incredibly easy, by using estimator objects. In this case we will import our estimator (the Multi-Layer Perceptron Classifier model) from the neural_network library of SciKit-Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an instance of the model, there are a lot of parameters you can choose to define and customize here, we will only define the hidden_layer_sizes. For this parameter you pass in a tuple consisting of the number of neurons you want at each layer, where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model. There are many ways to choose these numbers, but for simplicity we will choose 3 layers with the same number of neurons as there are features in our data set along with 500 max iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(3,3,3),max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been made we can fit the training data to our model, remember that this data has already been processed and scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "#print(\"Unknown label type: %s\" % repr(y_train))\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the output that shows the default values of the other parameters in the model. I encourage you to play around with them and discover what effects they have on your neural network in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Evaluation\n",
    "Now that we have a model it is time to use it to get predictions! We can do this simply with the predict() method off of our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use SciKit-Learn’s built in metrics such as a classification report and confusion matrix to evaluate how well our model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! This is pretty good considering how few lines of code we had to write for our neural network in Python. The downside however to using a Multi-Layer Perceptron model is how difficult it is to interpret the model itself. The weights and biases won’t be easily interpretable in relation to which features are important to the model itself.\n",
    "\n",
    "However, if you do want to extract the MLP weights and biases after training your model, you use its public attributes **coefs_** and **intercepts_**.\n",
    "\n",
    "**coefs_** is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1.\n",
    "\n",
    "**intercepts_** is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlp.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlp.coefs_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mlp.intercepts_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Hopefully you’ve enjoyed this brief discussion on Neural Networks! Try playing around with the number of hidden layers and neurons and see how they effect the results of your neural network in Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
